# US-Income-Distribution

## Seattle, Washington Housing Market Analysis

![alt text](https://github.com/Bunmi-Haastrup/Hermione-granger/blob/main/image.png)
       𝒫𝒾𝒸𝓉𝓊𝓇𝑒 𝓈𝑜𝓊𝓇𝒸𝑒:𝐿𝒶𝓃𝑔𝒶𝓃.𝒸𝑜𝓂, 𝑀𝒶𝓅 𝓈𝑜𝓊𝓇𝒸𝑒: 𝒯𝒶𝒷𝓁𝑒𝒶𝓊
      
# April 2021
Exploring, Wrangling, Visualisation and Analysis of Real Estate Data.

# Content

*[Project Outline](#project-outline)

*[Scenario](#scenario)

*[Objective](#objective)

*[Variables-description](#variables-description)

*[Data](#data)

*[Database](#database)

*[Visualisation](#visualisation)

*[Statistical Analysis](#statistical-analysis)

*[Conclusion](#conclusion)

# Project Outline
The project started by building a model that will predict the price of a house based on features provided in the dataset. The characteristics of the houses was also shown using some business intelligence tool such as Tableau. Some of iteretsing questions were answered through the assist of SQL in understanding the data challenge.

![alt text](https://github.com/petergeorge649/Hermione-granger/blob/main/ReadMe.png)

# Scenario
You are working as an analyst for a real estate company. Your company wants to build a machine learning model to predict the selling prices of houses based on a variety of features on which the value of the house is evaluated.

# Objective
To build a model that will predict the price of a house based on features provided in the dataset. Senior management also wants to explore the characteristics of the houses using some business intelligence tools. One of those parameters includes understanding which factors are responsible for higher property value - $650K and above.

# Variables-description

Id: Unique identification number for the property.

date: the date the house was sold.

price: the price of the house.

waterfront : the house which has a view to a waterfront.""

condition: How good the condition is (Overall). 1 indicates worn-out property and 5 excellent.

grade: Overall grade given to the housing unit, based on the King County grading system. 1 poor, 13 excellent.

Sqft_above: square footage of house apart from the basement.

Sqft_living15: Living room area in 2015(implies-- some renovations) This might or might not have affected the lotSize area.

Sqft_lot15: lotSize area in 2015(implies-- some renovations).

# Data
With a colossal sum of data accessible, we investigated a number of diverse sources. Hence CSV file was downloaded and prepared for the data analysis. Using Python we were able to conduct various steps to reach the analyis and reporting stage. we scratched a number of diverse tables into Pandas Dataframes and began to investigate and clean the data. After finalizing with the data cleaning, we sent out the clean dataframes to CSV files to be utilized for assist investigation in SQL.

# Database
Investigating the data given and considering approximately how to interface it together, we use an ERD for how we needed our database to see and act. Utilizing MySQL, we made a modern pattern, and imported the CSV record from the  Python Jupyter notebook  which enabled us to go further with analysis.

# Visualisation
Through the use of Tableau we were  able to effectively envision distinctive  plots such as the distribution plots of different variables (independent and dependent) and creation of a visually appealing dashboard to represent the information.

![alt text](https://github.com/petergeorge649/Hermione-granger/blob/main/tableau.png)

Tableau visualization for this project can be found under this link;

[link to tableau](https://public.tableau.com/profile/peter.george.ngugulu#!/vizhome/MidProjectworkongoing_16191675574620/WashingtonZipcodesbasedonPrice?publish=yes)

# Statistical Analysis
Under this stage we described the nature of the data to be analyzed, the given data were explored, the model to summarize understanding of how the data relates to the underlying dependent variable was created. This includes proving or disprove the validity of the model and lastly was to employ predictive analytics to anticipate future trends.

# Conclusion
Python was used to clean and sent out a few datasets. Utilizing the data at that point built a machine learning and use MySQL to meet our numerous objectives. As the data given was moderately straight forward, we had parcels of time to test with the visualisations in Scene through the use of Tableau , making a collection of outwardly engaging dashboards that offer curiously understanding.



